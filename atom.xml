<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Christopher Teng&#39;s Blog</title>
  
  <subtitle>关注技术、持续提升</subtitle>
  <link href="https://christopher-teng.github.io/atom.xml" rel="self"/>
  
  <link href="https://christopher-teng.github.io/"/>
  <updated>2021-05-20T13:21:39.815Z</updated>
  <id>https://christopher-teng.github.io/</id>
  
  <author>
    <name>滕飞</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Scrapy爬取诗词数据（终）</title>
    <link href="https://christopher-teng.github.io/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/"/>
    <id>https://christopher-teng.github.io/2021/05/04/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%88%EF%BC%89/</id>
    <published>2021-05-04T08:27:07.000Z</published>
    <updated>2021-05-20T13:21:39.815Z</updated>
    
    <content type="html"><![CDATA[<p>前文中，我们已经成功从诗词详情页爬取到了数据，接下来对数据内容根据需求进行加工处理以及存储。</p><p>首先，对标题进行处理，将其改为原标题加上正文第一句的形式。对数据的处理可以通过自定义管道来实现，scrapy 的管道可以将数据一层一层的进行复杂逻辑的处理，每一个管道类都需要返回 item，已保证数据可以在多个管道类之间流动。</p><p>使用 scrapy 创建爬虫项目时，会在项目目录下自动创建一个<code>pipelines.py</code>文件，其中已经预定义了一个和项目同名的管道类，我们就从这里开始编写数据处理逻辑：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">from</span> scrapy.exception <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .processors <span class="keyword">import</span> modify_title</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscCrawlerPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.logger=logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">        adapter = ItemAdapter(item)</span><br><span class="line">        <span class="keyword">if</span> adapter[<span class="string">&#x27;title&#x27;</span>] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> adapter[<span class="string">&#x27;content&#x27;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem()</span><br><span class="line">        adapter[<span class="string">&#x27;title&#x27;</span>] = modify_title(adapter[<span class="string">&#x27;title&#x27;</span>], adapter[<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line">        self.logger.debug(<span class="string">u&#x27;标题已成功经过修改，格式为：原标题 —— 诗文内容第一行 -- %(title)s&#x27;</span>, &#123;<span class="string">&#x27;title&#x27;</span>: adapter[<span class="string">&#x27;title&#x27;</span>]&#125;)</span><br><span class="line">        spider.crawler.stats.inc_value(<span class="string">&#x27;title_modify/modified&#x27;</span>, spider=spider)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><span id="more"></span><p>这里引入了<code>ItemAdapter</code>，这是 scrapy 提供的用于处理 item 的类，可以通过字段名获取 item 中的数据。</p><p>首先提取<code>title</code>字段和<code>content</code>字段的值，并且如果标题为空或者正文为空时，抛出一个<code>DropItem</code>，这是 scrapy 提供的用于丢弃数据的类，此处诗词数据如果没有标题或正文，则将该条数据抛弃。</p><p>然后调用在<code>processors.py</code>中自定义的标题处理方法<code>modify_title</code>，将处理后的返回值赋值给<code>title</code>字段以覆盖原标题，最后使用 python 的标准库<code>logging</code>打印日志，方便调试。</p><p><code>spider.crawler.stats.inc_value</code>是 scrapy 内置的状态记录方法，同样用于调试时查看爬虫运行情况。</p><p>下面是<code>modify_title</code>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modify_title</span>(<span class="params">title, content</span>):</span></span><br><span class="line">    first_line = re.match(<span class="string">r&#x27;(.+)(?=\n)&#x27;</span>, content).group()</span><br><span class="line">    title = title+<span class="string">&#x27; —— &#x27;</span>+first_line</span><br><span class="line">    <span class="keyword">return</span> title</span><br></pre></td></tr></table></figure><p>处理逻辑很简单，使用正则提取正文第一句，由于在 item 中定义<code>content</code>字段时，使用了<code>parse_content</code>方法把正文的每一个完整诗句进行换行，所以我们只需要匹配第一个换行符前面的内容就可以得到正文第一句。处理后返回的结果为：“原标题 —— 正文第一句”。</p><p>到此，可以运行爬虫爬取符合需求的诗词数据了，接下来对数据进行存储。最简单的存储方式就是在<code>settings.py</code>爬虫配置文件中，配置<code>FEEDS</code>字段，scrapy 默认支持的存储方式主要有：Local filesystem、FTP、S3、Google Cloud Storage 和 Standard output，这里我们配置为使用 JSON Lines 格式将数据存储到项目根目录下的<code>poems</code>目录下，文件名为<code>poems.jsonl</code>，该格式类似 JSON，使用逐行的方式存储数据，每行都是一个标准的 JSON 字符串，其优点一是方便追加数据，二是每一行都是一个标准的 JSON 格式字符串，所以使用的时候可以按行读取数据，避免文件过大，一次全部读取造成性能瓶颈。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FEEDS=&#123;</span><br><span class="line">    pathlib.Path(<span class="string">&#x27;poems/poems.jsonl&#x27;</span>):&#123;</span><br><span class="line">        <span class="string">&#x27;format&#x27;</span>:<span class="string">&#x27;jsonlines&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;encoding&#x27;</span>:<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>相比于直接存储与本地文件，更好的方式是使用数据库进行存储，下面在<code>pipelines.py</code>中编写一个<code>MongoDBPipeline</code>来通过自定义管道将数据存储进 MongoDB 中，在 python 中使用 mongodb 推荐使用 pymongo。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host=<span class="string">&#x27;127.0.0.1&#x27;</span>, port=<span class="number">27017</span>,username=<span class="string">&#x27;&#x27;</span>,password=<span class="string">&#x27;&#x27;</span>,auth_source=<span class="string">&#x27;&#x27;</span>, db_name=<span class="string">&#x27;zhsc_crawler&#x27;</span>, col_name=<span class="string">&#x27;poems&#x27;</span></span>):</span></span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.username=username</span><br><span class="line">        self.password=password</span><br><span class="line">        self.auth_source=auth_source</span><br><span class="line">        self.db_name=db_name</span><br><span class="line">        self.col_name=col_name</span><br><span class="line">        self.logger=logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls,crawler</span>):</span></span><br><span class="line">        _host = crawler.settings.get(<span class="string">&#x27;MONGO_HOST&#x27;</span>, <span class="string">&#x27;127.0.0.1&#x27;</span>)</span><br><span class="line">        _port = crawler.settings.getint(<span class="string">&#x27;MONGO_PORT&#x27;</span>, <span class="number">27017</span>)</span><br><span class="line">        _username=crawler.settings.get(<span class="string">&#x27;MONGO_USERNAME&#x27;</span>,<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">        _password=crawler.settings.get(<span class="string">&#x27;MONGO_PASSWORD&#x27;</span>,<span class="string">&#x27;123456&#x27;</span>)</span><br><span class="line">        _auth_source=crawler.settings.get(<span class="string">&#x27;MONGO_AUTHSOURCE&#x27;</span>,<span class="string">&#x27;admin&#x27;</span>)</span><br><span class="line">        _db_name = crawler.settings.get(<span class="string">&#x27;MONGO_DB&#x27;</span>, <span class="string">&#x27;zhsc_crawler&#x27;</span>)</span><br><span class="line">        _col_name = crawler.settings.get(<span class="string">&#x27;MONGO_COLLECTION&#x27;</span>, <span class="string">&#x27;poems&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(_host, _port,_username,_password,_auth_source, _db_name, _col_name)</span><br></pre></td></tr></table></figure><p>上面代码首先读取 mongodb 配置，准备好进行数据库连接，其中类方法<code>from_crawler</code>是 scrapy 提供的管道类中读取爬虫配置文件的方法。</p><p>在<code>settings.py</code>中配置 mongodb：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MONGO_HOST = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">MONGO_PORT = <span class="number">27017</span></span><br><span class="line">MONGO_USERNAME = <span class="string">&#x27;root&#x27;</span></span><br><span class="line">MONGO_PASSWORD = <span class="string">&#x27;123456&#x27;</span></span><br><span class="line">MONGO_AUTHSOURCE = <span class="string">&#x27;admin&#x27;</span></span><br><span class="line">MONGO_DB = <span class="string">&quot;zhsc_crawler&quot;</span></span><br><span class="line">MONGO_COLLECTION = <span class="string">&quot;poems&quot;</span></span><br></pre></td></tr></table></figure><p>MongoDB 的相关知识这里不单独做解释，网上资料很多，而且 MongoDB 上手也很简单。这里配置使用的数据库名称为<code>zhsc_crawler</code>，集合名称<code>poems</code>。</p><p>对数据库的连接和关闭可以放在运行爬虫和爬虫运行完毕时，这样可以有效节省开销：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">    self.connection = pymongo.MongoClient(host=self.host, port=self.port,username=self.username,password=self.password,authSource=self.auth_source)</span><br><span class="line">    self.db = self.connection[self.db_name]</span><br><span class="line">    self.collection = self.db[self.col_name]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">    self.connection.close()</span><br></pre></td></tr></table></figure><p><code>open_spider</code>和<code>close_spider</code>由 scrapy 提供，用于在蜘蛛启动和关闭时自定义操作。</p><p>最后是管道类必须实现的方法<code>process_item</code>，用于处理数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">    adapter = ItemAdapter(item)</span><br><span class="line">    self.collection.insert_one(<span class="built_in">dict</span>(item))</span><br><span class="line">    self.logger.debug(<span class="string">u&#x27;数据已插入MongoDB！ %(title)s -- %(times)s -- %(author)s&#x27;</span>,&#123;<span class="string">&#x27;title&#x27;</span>: adapter[<span class="string">&#x27;title&#x27;</span>], <span class="string">&#x27;times&#x27;</span>: adapter[<span class="string">&#x27;times&#x27;</span>], <span class="string">&#x27;author&#x27;</span>: adapter[<span class="string">&#x27;author&#x27;</span>]&#125;)</span><br><span class="line">    spider.crawler.stats.inc_value(<span class="string">&#x27;mongodb/inserted&#x27;</span>, spider=spider)</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>最后，在<code>settings.py</code>中配置<code>ITEM_PIPELINES</code>字段，开启自定义管道，默认只开启了创建项目时 scrapy 自动创建的项目同名管道类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.pipelines.ZhscCrawlerPipeline&#x27;</span>:<span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.pipelines.MongoDBPipeline&#x27;</span>:<span class="number">500</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的数值表示优先级，数值越小优先级越高，数据会依从优先级顺序在管道中传递进行处理。</p><p>到这里，爬虫开发已经接近完成，下面来考虑一些优化设置。</p><p>首先，一些大型网站，或者迭代周期很长的网站，他们的页面中可能存在着一些循环引用的链接，可能是无意造成的，也可能是故意设置，使爬虫陷入死循环，已进行反爬。因此我们因该在爬取时对重复的请求链接进行过滤。scrapy 默认开启了 RFPDupeFilter，通过生成一个 request_seen 文件记录请求指纹，然后在每次发出请求前先检查当前请求是否已经记录过，从而达到过滤重复请求的目的。</p><p>然后，当我们爬取的数据量非常大时，request_seen 文件体积将会不断增大，而运行爬虫时需要将该文件读入内存，这可能造成内存占用过大，最终导致程序崩溃。</p><p>一种办法是改成使用 Redis 来存储请求指纹，借助 Redis 的高性能，不需要一次性读取全部记录，从而改善默认 RFPDupeFilter 的缺陷，但是数据量太大时，使用 Redis 存储请求指纹仍然会占用大量内存。</p><p>这里采用另一种方式，Redis 结合布隆过滤器进行过滤。布隆过滤器的详细原理可以自行上网了解，布隆过滤器的优点是占用空间很少，缺点则是有一定的误判率，使用 Redis 内置的 bitset 可以方便的实现布隆过滤器，下面直接给出代码以及简单解释，在项目目录下新建<code>dupefilter.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"><span class="keyword">from</span> scrapy.dupefilters <span class="keyword">import</span> BaseDupeFilter</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.request <span class="keyword">import</span> request_fingerprint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .hashmap <span class="keyword">import</span> HashMap</span><br></pre></td></tr></table></figure><p>这里引入 scrapy 提供的 <code>BaseDupeFilter</code> 作为自定义 dupefilter 的基类，以及 <code>request_fingerprint</code> 方法生成请求指纹。</p><p>引入 python 标准库 <code>hashlib</code> 用于后面使用 md5 和 logging 生成日志。</p><p>自定义类<code>HashMap</code>用于生成布隆过滤器使用的哈希函数。</p><p>引入 redis 来连接操作 Redis。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,m,seed</span>):</span></span><br><span class="line">        self.m=m</span><br><span class="line">        self.seed=seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span>(<span class="params">self,value</span>):</span></span><br><span class="line">        ret=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(value)):</span><br><span class="line">            ret+=self.seed*ret+<span class="built_in">ord</span>(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m-<span class="number">1</span>)&amp;ret</span><br></pre></td></tr></table></figure><p>上面代码中，m 为布隆过滤器需要使用的位大小，seed 用于生成多个 hash 函数，hash 方法则把输入字符串映射到多个位，并且将对应位设置为 1。</p><p>下面编写基于 Redis 的布隆过滤器的主体代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisBloomDupeFilter</span>(<span class="params">BaseDupeFilter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, bitSize=<span class="number">32</span>, seeds=[<span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>], blockNum=<span class="number">1</span>, key=<span class="string">&#x27;bloomfilter&#x27;</span></span>):</span></span><br><span class="line">        self.redis = Redis(host=host, port=port, db=db)  <span class="comment"># 连接Redis</span></span><br><span class="line">        self.bitSize = <span class="number">1</span> &lt;&lt; bitSize  <span class="comment"># 在Redis中申请一个BitSet，Redis中BitSet实际上使用String进行存储，因此最大容量为512M，即2^32</span></span><br><span class="line">        self.seeds = seeds  <span class="comment"># 生成多个hash函数的种子</span></span><br><span class="line">        self.key = key  <span class="comment"># Redis中使用的键名</span></span><br><span class="line">        self.blockNum = blockNum  <span class="comment"># Redis中总共申请多少个BitSet</span></span><br><span class="line">        self.hashFunc = []  <span class="comment"># hash函数</span></span><br><span class="line">        <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds:</span><br><span class="line">            <span class="comment"># 根据提供的种子生成多个hash函数</span></span><br><span class="line">            self.hashFunc.append(HashMap(self.bitSize, seed))</span><br><span class="line">            self.logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span>(<span class="params">cls,settings</span>):</span></span><br><span class="line">        <span class="comment"># scrapy提供的dupefilter中读取爬虫配置的方法</span></span><br><span class="line">        _host = settings.get(<span class="string">&#x27;REDIS_HOST&#x27;</span>, <span class="string">&#x27;localhost&#x27;</span>)</span><br><span class="line">        _port = settings.getint(<span class="string">&#x27;REDIS_PORT&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line">        _db = settings.getint(<span class="string">&#x27;REDIS_DUPE_DB&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">        _bitSize = settings.getint(<span class="string">&#x27;BLOOMFILTER_BIT_SIZE&#x27;</span>, <span class="number">32</span>)</span><br><span class="line">        _seeds = settings.getlist(<span class="string">&#x27;BLOOMFILTER_HASH_SEEDS&#x27;</span>, [])</span><br><span class="line">        _blockNum = settings.getint(<span class="string">&#x27;BLOOMFILTER_BLOCK_NUMBER&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">        _key = settings.get(<span class="string">&#x27;BLOOMFILTER_REDIS_KEY&#x27;</span>, <span class="string">&#x27;bloomfilter&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(_host, _port, _db, _bitSize, _seeds, _blockNum, _key)</span><br></pre></td></tr></table></figure><p>在<code>settings.py</code>中加入 Redis 和布隆过滤器相应配置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">REDIS_HOST = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line">REDIS_DUPE_DB = <span class="number">0</span></span><br><span class="line">BLOOMFILTER_REDIS_KEY = <span class="string">&quot;bloomfilter&quot;</span></span><br><span class="line">BLOOMFILTER_BLOCK_NUMBER = <span class="number">1</span></span><br><span class="line">BLOOMFILTER_BIT_SIZE = <span class="number">31</span></span><br><span class="line">BLOOMFILTER_HASH_SEEDS = [<span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">31</span>, <span class="number">37</span>]</span><br></pre></td></tr></table></figure><p>接着完成<code>dupefilter.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_seen</span>(<span class="params">self, request</span>):</span></span><br><span class="line">    fp = request_fingerprint(request)</span><br><span class="line">    <span class="keyword">if</span> self.exists(fp):</span><br><span class="line">        <span class="comment"># 如果请求指纹已经存在</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.insert(fp)  <span class="comment"># 如果请求指纹不存在</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert</span>(<span class="params">self, str_input</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加入请求指纹</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    md5 = hashlib.md5()</span><br><span class="line">    md5.update(<span class="built_in">str</span>(str_input).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">    _<span class="built_in">input</span> = md5.hexdigest()</span><br><span class="line">    _name = self.key+<span class="built_in">str</span>(<span class="built_in">int</span>(_<span class="built_in">input</span>[<span class="number">0</span>:<span class="number">2</span>], <span class="number">16</span>) % self.blockNum)</span><br><span class="line">    <span class="keyword">for</span> func <span class="keyword">in</span> self.hashFunc:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将hash映射后的bit为置位为1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _offset = func.<span class="built_in">hash</span>(_<span class="built_in">input</span>)</span><br><span class="line">        self.redis.setbit(_name, _offset, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span>(<span class="params">self, str_input</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">判断请求指纹是否已存在</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> str_input:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">md5 = hashlib.md5()</span><br><span class="line">md5.update(<span class="built_in">str</span>(str_input).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">_<span class="built_in">input</span> = md5.hexdigest()</span><br><span class="line">_name = self.key+<span class="built_in">str</span>(<span class="built_in">int</span>(_<span class="built_in">input</span>[<span class="number">0</span>:<span class="number">2</span>], <span class="number">16</span>) % self.blockNum)</span><br><span class="line">ret = <span class="literal">True</span></span><br><span class="line"><span class="keyword">for</span> func <span class="keyword">in</span> self.hashFunc:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    如果经过hash映射之后对应的bit位上有任意一个0，则一定不存在</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    _offset = func.<span class="built_in">hash</span>(_<span class="built_in">input</span>)</span><br><span class="line">    ret = ret &amp; self.redis.getbit(_name, _offset)</span><br><span class="line"><span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">self.logger.debug(<span class="string">u&#x27;已过滤的重复请求：%(request)s&#x27;</span>, &#123;<span class="string">&#x27;request&#x27;</span>: request&#125;)</span><br><span class="line">spider.crawler.stats.inc_value(<span class="string">&#x27;redisbloomfilter/filtered&#x27;</span>, spider=spider)</span><br></pre></td></tr></table></figure><p>最后，在<code>settings.py</code>中开启自定义 dupefilter：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">&#x27;zhsc_crawler.dupefilter.RedisBloomDupeFilter&#x27;</span></span><br></pre></td></tr></table></figure><p>最后，在做一些反爬优化，首先可以通过为每个请求添加随机 User-Agent 来伪装不同客户端，要实现这一点可以通过scrapy的<code>Downloader Middleware</code>，下载器中间件位于downloader和scrapy engine之间，当engine通知downloader开始从指定url下载数据前，可以在下载器中间件中定义一些预操作。例如，先在<code>settings.py</code>中添加一组 UA，可以从网上找到很多 UA 信息，然后自定义一个<code>ZhscRandomUserAgentMiddleware</code>中间件类，每当downloader开始下载数据前，都对请求头设置一个随机UA：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscRandomUserAgentMiddleware</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, user_agents=[]</span>):</span></span><br><span class="line">        self.user_agents = user_agents</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(crawler.settings.getlist(<span class="string">&#x27;UA_Pool&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.user_agents <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(self.user_agents) &gt; <span class="number">0</span>:</span><br><span class="line">            request.headers.setdefault(<span class="string">b&#x27;User-Agent&#x27;</span>, random.choice(self.user_agents))</span><br></pre></td></tr></table></figure><p>其中，<code>from_crawler</code>是 scrapy 提供的读取配置文件的类方法，而<code>process_request</code>则是中间件类必须实现的请求处理方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">UA_Pool = [</span><br><span class="line">    <span class="comment"># User-Agent</span></span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Windows NT 10.0;Win64;x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36 Edg/89.0.774.76&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv,2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows NT 6.1; rv,2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent, Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent,Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.4094.1 Safari/537.36&#x27;</span>]</span><br></pre></td></tr></table></figure><p>然后在<code>settings.py</code>中配置使用自定义中间件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment">#    &#x27;zhsc_crawler.middlewares.ZhscCrawlerDownloaderMiddleware&#x27;: 543,</span></span><br><span class="line">    <span class="comment"># 发出请求前添加随机UA</span></span><br><span class="line">    <span class="string">&#x27;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">    <span class="string">&#x27;zhsc_crawler.middlewares.ZhscRandomUserAgentMiddleware&#x27;</span>: <span class="number">800</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里注意要先关闭 scrapy 默认的 UA 中间件。</p><p>最后，如果爬虫频繁发出大量请求的话，很容易被做了反爬的网站发现，因此可以通过在<code>settings.py</code>中对爬虫进行限速来预防，scrapy 提供了很方便的实现方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">60</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="comment"># each remote server</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">1.0</span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>到此，整个诗词数据爬虫开发便完成了，完整代码放在我的<a href="https://github.com/Christopher-Teng/poems_crawler.git">github 仓库</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前文中，我们已经成功从诗词详情页爬取到了数据，接下来对数据内容根据需求进行加工处理以及存储。&lt;/p&gt;
&lt;p&gt;首先，对标题进行处理，将其改为原标题加上正文第一句的形式。对数据的处理可以通过自定义管道来实现，scrapy 的管道可以将数据一层一层的进行复杂逻辑的处理，每一个管道类都需要返回 item，已保证数据可以在多个管道类之间流动。&lt;/p&gt;
&lt;p&gt;使用 scrapy 创建爬虫项目时，会在项目目录下自动创建一个&lt;code&gt;pipelines.py&lt;/code&gt;文件，其中已经预定义了一个和项目同名的管道类，我们就从这里开始编写数据处理逻辑：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; logging&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; itemadapter &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ItemAdapter&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; scrapy.exception &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; DropItem&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; .processors &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; modify_title&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ZhscCrawlerPipeline&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger=logging.getLogger(__name__)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;process_item&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self,item,spider&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        adapter = ItemAdapter(item)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;or&lt;/span&gt; adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;content&amp;#x27;&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;raise&lt;/span&gt; DropItem()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;] = modify_title(adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;], adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;content&amp;#x27;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.debug(&lt;span class=&quot;string&quot;&gt;u&amp;#x27;标题已成功经过修改，格式为：原标题 —— 诗文内容第一行 -- %(title)s&amp;#x27;&lt;/span&gt;, &amp;#123;&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;: adapter[&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;]&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        spider.crawler.stats.inc_value(&lt;span class=&quot;string&quot;&gt;&amp;#x27;title_modify/modified&amp;#x27;&lt;/span&gt;, spider=spider)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; item&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="网络爬虫" scheme="https://christopher-teng.github.io/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
    <category term="Scrapy" scheme="https://christopher-teng.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>使用Scrapy爬取诗词数据（续）</title>
    <link href="https://christopher-teng.github.io/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/"/>
    <id>https://christopher-teng.github.io/2021/05/03/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE%EF%BC%88%E7%BB%AD%EF%BC%89/</id>
    <published>2021-05-03T11:08:49.000Z</published>
    <updated>2021-05-20T13:21:39.815Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中，我们已经完成了对目标网站的分析(<a href="https://www.zhsc.net/">中华诗词网</a>)，接下来开始进行爬虫开发。</p><p>首先启动一个新项目，搭建基本目录结构，这里推荐使用 virtualenv 创建 python 虚拟环境进行开发。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir poems_crawler &amp;&amp; cd $_</span><br><span class="line"></span><br><span class="line">virtualenv -p Python3 venv</span><br><span class="line"></span><br><span class="line">. venv/bin/activate</span><br></pre></td></tr></table></figure><p>以上命令创建了一个 poems_crawler 空目录，并进入目录启激活 python3 虚拟环境。</p><p>接下来安装 scrapy 并创建一个 zhsc_crawler 的项目：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy3</span><br><span class="line"></span><br><span class="line">scrapy startproject zhsc_crawler</span><br></pre></td></tr></table></figure><p>到这里，我们使用 scrapy 新建了一个 zhsc_crawler 项目，在当前目录下多出了一个名为 zhsc_crawler 的项目根目录，进入项目目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd zhsc_crawler</span><br></pre></td></tr></table></figure><p>查看目录结构可以看到在项目根目录下还有一个同名的目录<code>zhsc_crawler</code>，以及一个名为 scrapy.cfg 的 scrapy 配置文件。这里的<code>zhsc_crawler</code>目录就是爬虫项目的包目录，这里也是之后我们自己编写代码的地方，其下包含了 items.py(Item 定义文件)、middlewares.py(自定义中间件)、pipelines.py(自定义管道)、settings.py(项目配置文件)和一个名为<code>spiders</code>的目录，这是放置爬虫类的包目录。</p><span id="more"></span><p>现在来创建我们的蜘蛛类：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider poems www.zhsc.net</span><br></pre></td></tr></table></figure><p>现在在<code>spiders</code>目录下会生成一个 poems.py 文件，我们在这里来编写蜘蛛。</p><p>首先修改其中的<code>start_urls</code>，也就是爬虫开始发起请求的地址，根据前面的分析，种子页应该是作者列表页：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls=[<span class="string">&#x27;https://www.zhsc.net/Index/shi_more.html&#x27;</span>]</span><br></pre></td></tr></table></figure><p>现在来编写处理种子页响应的代码，在<code>poems.py</code>中，对<code>start_urls</code>发起的请求，其响应会被<code>parse</code>方法处理，因此先来看看在<code>parse</code>方法里面怎么写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">author=<span class="built_in">getattr</span>(self,<span class="string">&#x27;author&#x27;</span>,<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>前面说过，爬虫将被设计为爬取指定作者的诗词数据，在 scrapy 中提供了通过 scrapy cli 向蜘蛛中动态传入参数的功能，其方式为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl poems -a author=李白</span><br></pre></td></tr></table></figure><p>通过<code>-a</code>选项传入的参数会被传递给蜘蛛类的<code>__init__</code>方法成为蜘蛛实例上的一个属性，因此通过 getattr 方法便可以获取由命令行传入的参数。</p><p>下一步，对作者列表进行筛选，匹配指定的作者。</p><p>首先因该从页面中提取所有作者的链接，在 scrapy 中可以使用 xpath 和 css 两种方式来获取 html 内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">author_links=response.css(<span class="string">&#x27;.ci_lei1&gt;.ci_lei1&gt;.ci_lei1_xuan&gt;.ci_lei1_xuan2 a&#x27;</span>).getall()</span><br></pre></td></tr></table></figure><p><code>response.css</code>是使用 scrapy 的 css 选择器获取 html 内容，得到的结果可以通过<code>get</code>和<code>getall</code>方法来取得第一项匹配结果或所有匹配结果的列表。这里使用<code>getall</code>得到当前页面包含的所有作者链接。</p><p>接下来从所有作者中匹配指定的作者：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> author <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">  <span class="keyword">for</span> link <span class="keyword">in</span> author_links:</span><br><span class="line">    poems_list_page_url=find_author_url(link,author)</span><br><span class="line">    <span class="keyword">if</span> poems_list_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">yield</span> response.follow(poems_list_page_url,self.parse_list)</span><br><span class="line"><span class="keyword">return</span> ZhscCrawlerItem()</span><br></pre></td></tr></table></figure><p>这里我们使用<code>for...in</code>遍历作者列表来进行匹配，这里引入的自定义的方法<code>find_author_url</code>，如果调用该方法成功返回了链接地址，则使用 scrapy 提供的<code>response.follow</code>方法让蜘蛛沿着指定的链接继续爬取，最后<code>return ZhscCrawlerItem()</code>是返回爬取到的数据，这是新建爬虫项目时，scrapy 自动为我们在<code>items.py</code>中生成的数据类，稍后我们会在这里对爬取数据进行定义。</p><p>先来看看自定义的<code>find_author_url</code>方法，其定义在项目目录下的<code>processors.py</code>中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_author_url</span>(<span class="params">link,author</span>):</span></span><br><span class="line">  <span class="keyword">if</span> re.search(<span class="string">r&#x27;&gt;\s*&#123;&#125;\s*&lt;&#x27;</span>.<span class="built_in">format</span>(author),link) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">return</span> re.search(<span class="string">r&#x27;(?:href\s*\=\s*&quot;)(.+?)(?:&quot;)&#x27;</span>,link).group(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>逻辑很简单，使用正则先匹配指定作者，如果找到则返回其链接，否则返回<code>None</code>。</p><p>现在如果我们传入作者名字并且假定这是一个被收录在目标网站中的作者，那么蜘蛛将会进入诗词列表页面，下面来看看怎么编写处理诗词列表页面的方法<code>parse_list</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">detail_urls=response.css(<span class="string">&#x27;.zh_sou_jie&gt;.zh_jie_con a::attr(href)&#x27;</span>).getall()</span><br></pre></td></tr></table></figure><p>scrapy 的 css 选择器使用<code>::attr</code>的语法来提取 html 标签中的属性，这里首先提取当前诗词列表页面中的所有诗词详情页地址。然后让蜘蛛跟随详情页链接，进入下一步详情页爬取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> detail_urls:</span><br><span class="line">  <span class="keyword">yield</span> response.follow(url,self.parse_item)</span><br></pre></td></tr></table></figure><p>之前分析过，诗词详情由于条目众多，目标网站使用了分页器，因此在诗词列表页的处理中还因该让蜘蛛跟随分页器的链接继续爬取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_page_urls=response.css(<span class="string">&#x27;.page a::attr(href)&#x27;</span>).getall()</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> next_page_urls:</span><br><span class="line">  <span class="keyword">yield</span> response.follow(url,self.parse_list)</span><br></pre></td></tr></table></figure><p>到这一步，蜘蛛应该可以顺利到达所有的诗词详情页，接下来编写蜘蛛的核心代码，对数据页的爬取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">  loader=ItemLoader(item=ZhscCrawlerItem(),response=response)</span><br></pre></td></tr></table></figure><p>这里使用了两个新类<code>ItemLoader</code>和<code>ZhscCrawlerItem</code>，上面提到过，<code>ZhscCrawlerItem</code>是新建爬虫时 scrapy 自动为我们创建的数据类，而<code>ItemLoader</code>则是 scrapy 为我们提供的数据处理类，可以直接引入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br></pre></td></tr></table></figure><p>首先来定义数据类，根据前文分析，要爬取的数据主要包含标题 title、年代 times、作者 author 和正文 content，在项目目录下的<code>items.py</code>中定义数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itemloaders.processors <span class="keyword">import</span> Join,MapCompose,TakeFirst</span><br><span class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Field,Item</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .processors <span class="keyword">import</span> get_author,get_times,parse_content</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhscCrawlerItem</span>(<span class="params">Item</span>):</span></span><br><span class="line">  title=Field(input_processor=MapCompose(<span class="built_in">str</span>.strip,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  times=Field(input_processor=MapCompose(get_times,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  author=Field(input_processor=MapCompose(get_author,stop_on_none=<span class="literal">True</span>),output_processor=TakeFirst())</span><br><span class="line">  content=Field(input_processor=MapCompose(parse_content,stop_on_none=<span class="literal">True</span>),output_processor=Join(<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure><p>scrapy 中定义数据结构使用<code>Item</code>和<code>Field</code>基类，使用语法<code>字段名=Field()</code>来规定数据因该包含的字段。</p><p>前文中已经分析过，对详情页中原始的标题、年代、作者和正文数据因该先进行处理，让我们回顾一下：</p><ol><li>标题要修改为原始标题加上正文内容的第一句</li><li>年代和作者信息位于同一个<code>&lt;p&gt;</code>标签内，要分别进行提取</li><li>正文结构不尽相同，有些诗词正文中还带有注释内容，需要进行过滤</li></ol><p>在scrapy中当我们要爬取指定页面时，首先会由spider发起一个爬虫请求给scrapy engine，然后engine将请求添加到scheduler的调度队列，当接到scheduler的响应后engine通知downloader对指定url开始下载数据，完成下载之后，将数据交给spider解析。</p><p>所以对于上面需要进行的数据处理，就可以放在spider对downloader下载回来的数据进行解析的时候。</p><p>在<code>Field</code>中，使用<code>input_processor</code>和<code>output_processor</code>就可以分别在downloader下载完数据时进行处理，以及在spider解析完数据向后传递给pipeline之前进行处理。而<code>itemloaders.processors</code>中提供了<code>Join</code>、<code>MapCompose</code>和<code>TakeFirst</code>三个数据处理方法，其中<code>Join</code>和<code>TakeFirst</code>用于<code>output_processor</code>，顾名思义，这两个方法分别用于将数据列表中的每一项合并后输出，和获取数据列表中的第一项输出。</p><p><code>MapCompose</code>方法用于<code>input_processor</code>，可以传入多个方法，<code>MapCompose</code>会将获取的数据依次传递给这些方法处理，然后将结果交给向后传递。由于获取的原始数据可能是一个列表，比如上面的诗词正文，在详情页中，正文是位于一个<code>&lt;div&gt;</code>标签下的多个文本节点组成的，因此通过 scrapy 的 css 选择器得到的是一个列表，假如列表中某一项为空值，而我们指定的处理方法可能无法处理输入值为空的情况，这是便会报错，所以可以在<code>MapCompose</code>中指定<code>stop_on_none=True</code>来规定遇到空值时停止继续处理。</p><p><code>get_author</code>、<code>get_times</code>和<code>parse_content</code>是<code>processors.py</code>中的三个自定义的处理函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_author</span>(<span class="params">value</span>):</span></span><br><span class="line">  result=re.search(<span class="string">r&#x27;(?:作者:\s*)(.*)(?:\s*)&#x27;</span>,value)</span><br><span class="line">  <span class="keyword">return</span> result.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_times</span>(<span class="params">value</span>):</span></span><br><span class="line">  result=re.search(<span class="string">r&#x27;(?:年代:\s*)(.+?)(?:\s)&#x27;</span>,value)</span><br><span class="line">  <span class="keyword">return</span> result.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_content</span>(<span class="params">value</span>):</span></span><br><span class="line">  value=remove_tags(value.strip())</span><br><span class="line">  <span class="keyword">return</span> re.sub(<span class="string">r&#x27;(。|！|？)&#x27;</span>,<span class="string">r&#x27;\1\n&#x27;</span>,value)</span><br></pre></td></tr></table></figure><p>这里使用了 python 标准库<code>w3lib</code>里面的 remove_tags 方法去除 html 标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br></pre></td></tr></table></figure><p><code>re.sub(r&#39;(。|！|？)&#39;,r&#39;\1\n&#39;,value)</code>在句号或感叹号或问号后面添加一个换行，这是为了符合阅读诗词时的习惯：在一个完整的诗句后面换行。</p><p>在这里并没有对标题进行处理(将标题修改为原始标题加上正文第一句)，因为在这里主要是定义数据结构，使用<code>input_processor</code>和<code>output_processor</code>的目的是正确提取数据，规范数据内容，而对标题的处理属于项目设计的需求，因此将放到后面自定义管道中再进行。</p><p>回到蜘蛛<code>poems.py</code>中，现在处理详情页的方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">  loader=ItemLoader(item=ZhscCrawlerItem(),response=response)</span><br></pre></td></tr></table></figure><p>接着往下，向<code>loader</code>中传入定义的字段数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loader.add_css(<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;span::text,.zh_shi_xiang1&gt;span&gt;*::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;times&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;p::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;author&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;p::text&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;.zh_shi_xiang1&gt;div::text,.zh_shi_xiang1&gt;div&gt;*::text&#x27;</span>)</span><br><span class="line"><span class="keyword">return</span> loader.load_item()</span><br></pre></td></tr></table></figure><p><code>loader.add_css</code>方法使用 scrapy 的 css 选择器将 html 中的内容添加到在<code>ZhscCrawlerItem</code>中定义的指定字段，最后使用<code>loader.load_item</code>方法将获取的数据返回，这里要注意使用<code>add_css</code>方法后，只是 item 的指定字段注入数据，但是数据并不会被返回，所以在完成向所有字段注入数据后，一定要调用<code>load_item</code>方法将数据返回。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在上一篇文章中，我们已经完成了对目标网站的分析(&lt;a href=&quot;https://www.zhsc.net/&quot;&gt;中华诗词网&lt;/a&gt;)，接下来开始进行爬虫开发。&lt;/p&gt;
&lt;p&gt;首先启动一个新项目，搭建基本目录结构，这里推荐使用 virtualenv 创建 python 虚拟环境进行开发。&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mkdir poems_crawler &amp;amp;&amp;amp; cd $_&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;virtualenv -p Python3 venv&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;. venv/bin/activate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;以上命令创建了一个 poems_crawler 空目录，并进入目录启激活 python3 虚拟环境。&lt;/p&gt;
&lt;p&gt;接下来安装 scrapy 并创建一个 zhsc_crawler 的项目：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install scrapy3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scrapy startproject zhsc_crawler&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;到这里，我们使用 scrapy 新建了一个 zhsc_crawler 项目，在当前目录下多出了一个名为 zhsc_crawler 的项目根目录，进入项目目录：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd zhsc_crawler&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;查看目录结构可以看到在项目根目录下还有一个同名的目录&lt;code&gt;zhsc_crawler&lt;/code&gt;，以及一个名为 scrapy.cfg 的 scrapy 配置文件。这里的&lt;code&gt;zhsc_crawler&lt;/code&gt;目录就是爬虫项目的包目录，这里也是之后我们自己编写代码的地方，其下包含了 items.py(Item 定义文件)、middlewares.py(自定义中间件)、pipelines.py(自定义管道)、settings.py(项目配置文件)和一个名为&lt;code&gt;spiders&lt;/code&gt;的目录，这是放置爬虫类的包目录。&lt;/p&gt;</summary>
    
    
    
    <category term="网络爬虫" scheme="https://christopher-teng.github.io/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
    <category term="Scrapy" scheme="https://christopher-teng.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>使用Scrapy爬取诗词数据</title>
    <link href="https://christopher-teng.github.io/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/"/>
    <id>https://christopher-teng.github.io/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/</id>
    <published>2021-05-02T14:53:56.000Z</published>
    <updated>2021-05-20T13:21:39.795Z</updated>
    
    <content type="html"><![CDATA[<p>在本篇中我将使用 Scrapy 来开发一个简单的文本爬虫，从<a href="https://www.zhsc.net/">中华诗词网</a>上爬取诗词数据。</p><p>首先对目标网站进行分析，在其主页右下角有一个历代作者的链接入口，从该链接可以进入作者列表页面，如下图中红圈所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/index.png" class=""><span id="more"></span><p>接下来进入作者列表页面，可以看见在页面下方有一个“历代作者”面板区域，其中按历史时期进行划分，收录了各个时代的文人，如下图红圈所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/authorlist.png" class=""><p>打开开发者面板对页面内容进行分析，可以看到该页面包含了所有作者的链接，只不过页面上仅显示当前选中历史时期下的作者列表，而未选中的部分默认使用<code>style=&quot;display:none;&quot;</code>隐藏了起来，如下图红圈所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/dom.png" class=""><p>仔细浏览以下上面的作者列表页可以发现，中华诗词网收录的文人非常多，而每个人又有很多诗词作品，如果一次全部爬取数据量太大，不仅耗时，而且在不使用代理 IP 的情况下，容易被封，更重要的是，对于传统诗词人们的学习和关注重心都在历史上的著名文人上，而对于全部收录数据来说，这些著名文人的作品只是一小部分。</p><p>因此，我将爬虫设计为只对指定的作者进行爬取，而要实现这一点，正好可以从上面的作者列表页作为种子页面启动爬虫。从该页中的所有作者条目中匹配出指定的作者，然后再链接到诗词列表页面，如下图所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/poemlist.png" class=""><p>从诗词列表中，就可以通过每一项的链接到达我们真正要爬取数据的诗词详情页，同时由于某些文人作品很多，采用了分页，因此还需要匹配分页器中的链接地址，依次从所有诗词列表页面中达到详情页爬取数据。</p><p>这里需要注意，很多作者都有和别人同名的作品，而且某个作者自己也可能有很多同名的作品，这种情况尤其常见于宋词当中：词牌名相同，正文不同，如上图中红圈所示，柳永名下同为“黄莺儿”的词作便有两首。这里便要考虑爬取数据之后，怎样存取才更方便日后使用的时候查询数据，在这里我使用标识宋词常用的方法：标题=词牌名+正文首句。</p><p>最后，来分析我们最终需要爬取数据的诗词详情页，如下图所示，需要爬取的数据有：标题、年代、作者和正文：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/poem.png" class=""><p>打开开发者工具，仔细分析页面结构，可以看出，需要爬取的数据全部位于<code>class=&quot;shi_neirong&quot;</code>的 <code>&lt;div&gt;</code> 标签下的<code>class=&quot;zh_shi_xiang1&quot;</code>的 <code>&lt;div&gt;</code> 标签内，该 <code>&lt;div&gt;</code> 标签包括一个 <code>&lt;span&gt;</code> 标签、一个 <code>&lt;p&gt;</code> 标签和一个 <code>&lt;div&gt;</code> 标签，其中 <code>&lt;span&gt;</code> 标签中的是标题，<code>&lt;p&gt;</code> 标签中则同时包含了年代和作者，而 <code>&lt;div&gt;</code> 标签中的是正文，如下图所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/poem1dom.png" class=""><p>接下来在随机打开多个详情页，确认页面结构是否符合上面的分析，这里为了节省篇幅，只列举出一个和上面详情页稍有不同的页面，如下图所示：</p><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/poem2.png" class=""><img src="/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/poem2dom.png" class=""><p>首先从页面上观察可以发现两处不同，一是正文内容中出现了<code>&lt;p&gt;</code>，二是正文中多了很多蓝色的文字。</p><p>打开开发者工具分析，首先从页面整体结构上看，数据所在的标签位置和层次关系仍然符合上面的分析，如图中横线所示。其次，第一个不同点，多出来的<code>&lt;p&gt;</code>应该是网站后台数据处理错误，将 html 的 <code>&lt;p&gt;</code> 标签以文本存入了诗词数据中，那么我们之后爬取数据时，应该要对数据进行去 html 标签的处理，第二个不同点，可以看出是对诗文内容的注解，以<code>&lt;span&gt;</code>标签的形式插入到正文所在的<code>&lt;div&gt;</code>标签中，注解不是我们需要的数据，之后爬取时应该过滤掉，如图中红圈所示。</p><p>到这里，对于目标网站的分析基本完成，那么下面开始着手编写爬虫。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在本篇中我将使用 Scrapy 来开发一个简单的文本爬虫，从&lt;a href=&quot;https://www.zhsc.net/&quot;&gt;中华诗词网&lt;/a&gt;上爬取诗词数据。&lt;/p&gt;
&lt;p&gt;首先对目标网站进行分析，在其主页右下角有一个历代作者的链接入口，从该链接可以进入作者列表页面，如下图中红圈所示：&lt;/p&gt;
&lt;img src=&quot;/2021/05/02/%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96%E8%AF%97%E8%AF%8D%E6%95%B0%E6%8D%AE/index.png&quot; class=&quot;&quot;&gt;</summary>
    
    
    
    <category term="网络爬虫" scheme="https://christopher-teng.github.io/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
    <category term="Scrapy" scheme="https://christopher-teng.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>通过动态规划算法解决618最佳购物方案</title>
    <link href="https://christopher-teng.github.io/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/"/>
    <id>https://christopher-teng.github.io/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/</id>
    <published>2021-04-05T11:45:04.000Z</published>
    <updated>2021-05-20T13:21:39.819Z</updated>
    
    <content type="html"><![CDATA[<p>最近刚装修完新房，新家种种东西方方面面都亟待采买，正好还有两个多月便是 618，打算趁着促销活动，购置一批生活家电。</p><p>看着各大网购平台上琳琅满目的商品，各种宣传导购、直播带货，感觉每样东西不是吊炸天便是亮瞎眼，恨不能来者不拒，买买买…，奈何囊中羞涩，为了不至于“一波买买买，稀饭喝到腿发软”，只好提前计划，理性消费。</p><p>首先规划好消费总额，暂定为只使用信用卡额度的 70%。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TOTAL_ASSETS = <span class="number">10000</span>  <span class="comment"># 总资产</span></span><br><span class="line">RATE = <span class="number">0.7</span>  <span class="comment"># 使用百分比</span></span><br><span class="line">AVAILABLE_ASSET = TOTAL_ASSETS*RATE  <span class="comment"># 可用资产</span></span><br></pre></td></tr></table></figure><p>接下来需要列出商品清单，这里要考虑好以什么作为参考来判断最后得出的购买清单是最优的，以及以什么作为参数来选择商品。</p><p>首先，既然限制了消费总额，因此使用商品的价格来作为选择与否的依据；其次，对于每样商品，会有一个需求度的期望值，也就是说该商品是否急需使用。举个例子，如果将需求度按 1 到 10 来划分，冰箱的需求度是 10(家里面总是会有东西需要冷藏)，而破壁机的需求度是 5(平时做点鲜果汁、热豆浆什么的，营养又健康，但是没有的话对生活影响不会太大)。那么，最后的购买清单便以尽可能达到最大需求度为最优解。</p><p>下面列出商品清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">COMMODITY_LIST = &#123;</span><br><span class="line">    <span class="string">&#x27;苏泊尔电饭煲&#x27;</span>: (<span class="number">319</span>, <span class="number">10</span>),</span><br><span class="line">    <span class="string">&#x27;海尔净水器&#x27;</span>: (<span class="number">1188</span>, <span class="number">8</span>),</span><br><span class="line">    <span class="string">&#x27;海尔智能门锁&#x27;</span>: (<span class="number">1399</span>, <span class="number">9</span>),</span><br><span class="line">    <span class="string">&#x27;九阳破壁机&#x27;</span>: (<span class="number">449</span>, <span class="number">5</span>),</span><br><span class="line">    <span class="string">&#x27;海尔智能音箱&#x27;</span>: (<span class="number">199</span>, <span class="number">3</span>),</span><br><span class="line">    <span class="string">&#x27;海尔冰箱&#x27;</span>: (<span class="number">3199</span>, <span class="number">10</span>),</span><br><span class="line">    <span class="string">&#x27;海尔滚筒洗衣机&#x27;</span>: (<span class="number">2699</span>, <span class="number">10</span>),</span><br><span class="line">    <span class="string">&#x27;格力空气循环扇&#x27;</span>: (<span class="number">399</span>, <span class="number">6</span>),</span><br><span class="line">    <span class="string">&#x27;苏泊尔电压力锅&#x27;</span>: (<span class="number">259</span>, <span class="number">7</span>),</span><br><span class="line">    <span class="string">&#x27;碧然德滤水壶&#x27;</span>: (<span class="number">219</span>, <span class="number">5</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>动态规划的原理是从小问题着手，然后逐步解决大问题。在这里，可以对总金额进行划分，比如先假设只能使用 100 块，然后找出当前的最佳购买方式，接着再假设只能使用 200 块，此时已经知道了使用 100 块时的最佳购买方式，只需要在此基础上看看使用 200 块能多买到什么东西，然后和上一次的计算结果进行比较，就可以轻松得出可以使用 200 块时的最佳购买方式，以此类推。</p><span id="more"></span><p>使用动态规划通常都会伴随着使用网格。首先建立一张表格，其中每一行都对应一种商品，每一列对应当前假设可以使用的金额，单元格中的数值就是获得的需求值。每个单元格的初始值为 0，即当前还没有选择购买任何商品。这里为了方便演示，只列出前 10 列，即最大可用金额为 1000，如下图：</p><img src="/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/new.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_shopping_list</span>(<span class="params">money, commodities</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    指定可用金额作为约束条件，从提供的可选商品中选择出最佳选择方式，获取最多的需求值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    current_asset = [</span><br><span class="line">        i+<span class="number">100</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">int</span>(money), <span class="number">100</span>)]  <span class="comment"># 将可用资产以100为最小单位进行划分</span></span><br><span class="line">    commodity_names = <span class="built_in">list</span>(commodities.keys())  <span class="comment"># 所有可选商品</span></span><br><span class="line">    selected_goods = [[<span class="built_in">set</span>() <span class="keyword">for</span> _ <span class="keyword">in</span> current_asset]</span><br><span class="line">                      <span class="keyword">for</span> _ <span class="keyword">in</span> commodity_names]  <span class="comment"># 每个单元格中选中的商品，初始为空</span></span><br><span class="line">    table = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> current_asset]</span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> commodity_names]  <span class="comment"># 用于动态规划计算的网格</span></span><br></pre></td></tr></table></figure><p>现在，可以开始进行动态规划计算：</p><ol><li><p>首先，查看当前行对应商品的价格，将其与每一列的可用金额进行对比，这里可能出现两种情况：</p><ol><li><p>当前可用金额小于商品价格：</p><p>这种情况下，不能选择该商品，那么单元格中的值就应该是上一次计算所得。对于第一行，也就是第一次计算来说，保持单元格中的值不动。</p></li><li><p>当前可用金额大于等于商品价格：</p><p>这种情况下，可以选择购买该商品，让我们来计算可以获得多少需求值：</p><ol><li><p>第一步，查看该商品的需求值<strong>“当前商品需求值”</strong></p></li><li><p>第二步，计算余额，即如果选择购买该商品，还剩多少钱可用，<strong>“当前余额”</strong> = <strong>“当前可用金额”</strong> - <strong>“当前商品价格”</strong></p></li><li><p>第三步，计算余额可获得的需求值，<strong>“余额可得需求值”</strong> = <strong>“上一行”</strong>和<strong>“最接近余额的可用金额所在列”</strong>对应的单元格中的需求值。</p><p>举例来说，假设现在计算第四行“九阳破壁机（449，5）”和最后一列“￥ 1000.00”，那么购买破壁机后，余额等于 1000 - 449 = 551，那么余额可得需求值应该查找第三行第 5 列(￥ 500.00)单元格中的值。假如我们现在计算的是第四行第五列，那么余额等于 500 - 449 = 51，而最小可用金额是 100，说明余额已经不足最小可计算的可用金额，那么余额可得需求值就为 0。类似的，当第一次计算时，选择当前商品后，无论剩多少钱，都没有东西可选，即余额可得需求值也为 0。</p></li><li><p>第四步，计算选择该商品后可以获得的需求值，<strong>“选择当前商品可得需求值”</strong> = <strong>“当前商品需求值”</strong> + <strong>“余额可得需求值”</strong></p></li><li><p>第五步，查看当前可用金额下上一次计算的结果，<strong>“上一次计算结果”</strong> = **”本列上一行”**对应单元格中的需求值</p><p>当计算第一行时，即第一次计算，上一次计算结果可以认为是 0，即单元格中的初始需求值。</p></li><li><p>最后一步，比较<strong>“选择当前商品可得需求值”</strong>和<strong>“上一次计算结果”</strong>，如果<strong>“上一次计算结果需求值更大”</strong>，则将当前单元格更新为<strong>“上一次计算结果”</strong>，否则，将当前单元格更新为<strong>“选择当前商品可得需求值”</strong>。这里注意要同步更新选择的商品。</p></li></ol><img src="/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/step1.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_cells</span>(<span class="params">row, column</span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   更新单元格的方法，传入当前单元格的行、列数</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   current_commodity = commodity_names[row]  <span class="comment"># 当前商品名</span></span><br><span class="line">   (price, grade) = commodities[current_commodity]  <span class="comment"># 当前商品价格和需求值</span></span><br><span class="line">   asset = current_asset[column]  <span class="comment"># 当前可用金额</span></span><br><span class="line">   <span class="keyword">if</span> row == <span class="number">0</span>:  <span class="comment"># 第一次计算的时候</span></span><br><span class="line">         <span class="keyword">if</span> asset &gt;= price:  <span class="comment"># 当前可用金额大于等于当前商品价格</span></span><br><span class="line">            table[row][column] = grade  <span class="comment"># 更新当前单元格数值</span></span><br><span class="line">            selected_goods[row][column].add(</span><br><span class="line">               current_commodity)  <span class="comment"># 更新当前单元格选择商品</span></span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">         <span class="keyword">if</span> asset &gt;= price:  <span class="comment"># 当前可用金额大于等于当前商品价格</span></span><br><span class="line">            remaining_asset = asset-price  <span class="comment"># 剩余金额</span></span><br><span class="line">            <span class="comment"># 最大的剩余可用金额，这里比如手中还剩371，则对应最大的剩余可用金额是300</span></span><br><span class="line">            max_remaining_available_asset = (remaining_asset//<span class="number">100</span>)*<span class="number">100</span></span><br><span class="line">            remaining_asset_index = -<span class="number">1</span> <span class="keyword">if</span> max_remaining_available_asset == <span class="number">0</span> <span class="keyword">else</span> current_asset.index(</span><br><span class="line">               max_remaining_available_asset)</span><br><span class="line">            <span class="comment"># 剩余金额可以获得的需求值</span></span><br><span class="line">            remainder_grade = table[commodity_index -</span><br><span class="line">                                    <span class="number">1</span>][remaining_asset_index] <span class="keyword">if</span> remaining_asset_index &gt; -<span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            current_total_grade = grade+remainder_grade  <span class="comment"># 当前商品需求值和剩余金额可获得需求值之和</span></span><br><span class="line">            <span class="comment"># 本列上一行单元格的值，即当前可用金额下，前一次计算所得的最大需求值</span></span><br><span class="line">            prev_grade = table[row-<span class="number">1</span>][column]</span><br><span class="line">            <span class="keyword">if</span> current_total_grade &gt;= prev_grade:  <span class="comment"># 更新当前单元格</span></span><br><span class="line">               table[row][column] = current_total_grade</span><br><span class="line">               <span class="keyword">if</span> remaining_asset_index &gt; -<span class="number">1</span>:  <span class="comment"># 选择当前商品后，并且剩余金额大于等于100</span></span><br><span class="line">                     prev_selected_goods = selected_goods[row -</span><br><span class="line">                                                         <span class="number">1</span>][remaining_asset_index].copy()</span><br><span class="line">                     prev_selected_goods.add(current_commodity)</span><br><span class="line">                     <span class="comment"># 更新当前单元格选择的商品</span></span><br><span class="line">                     selected_goods[row][column] = prev_selected_goods</span><br><span class="line">               <span class="keyword">else</span>:  <span class="comment"># 选择当前商品后，剩余金额不足100</span></span><br><span class="line">                     selected_goods[row][column] = &#123;</span><br><span class="line">                        current_commodity&#125;</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">               table[row][column] = table[row-<span class="number">1</span>][column]</span><br><span class="line">               selected_goods[row][column] = selected_goods[row -</span><br><span class="line">                                                            <span class="number">1</span>][column].copy()</span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 当前可用金额小于当前商品价格时</span></span><br><span class="line">            <span class="comment"># 更新当前单元格需求值为上一次计算结果</span></span><br><span class="line">            table[row][column] = table[row-<span class="number">1</span>][column]</span><br><span class="line">            <span class="comment"># 更新当前单元格选择商品为上一次计算结果</span></span><br><span class="line">            selected_goods[row][column] = selected_goods[row -</span><br><span class="line">                                                         <span class="number">1</span>][column].copy()</span><br></pre></td></tr></table></figure></li></ol></li><li><p>查看下一行，并且依照上面的方法更新单元格。</p><img src="/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/step2.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> commodity_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(commodity_names)):  <span class="comment"># 遍历每一行，即每种可选商品</span></span><br><span class="line">   <span class="keyword">for</span> asset_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(current_asset)):  <span class="comment"># 遍历每一列，即每一档可用金额</span></span><br><span class="line">      update_cells(commodity_index, asset_index)  <span class="comment"># 更新单元格</span></span><br></pre></td></tr></table></figure></li></ol><p>最终，当我们更新完最后一个单元格时，整个动态规划算法结束，最后一个单元格的需求值就是我们求得的最优解，对应选择的商品就是最佳购物方式。</p><img src="/2021/04/05/%E9%80%9A%E8%BF%87%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3618%E6%9C%80%E4%BD%B3%E8%B4%AD%E7%89%A9%E6%96%B9%E6%A1%88/last.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> &#123;<span class="string">&#x27;选择的商品&#x27;</span>: selected_goods[-<span class="number">1</span>][-<span class="number">1</span>], <span class="string">&#x27;获得的需求值&#x27;</span>: table[-<span class="number">1</span>][-<span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure><p>最终计算出的结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;选择的商品&#x27;</span>: &#123;<span class="string">&#x27;海尔滚筒洗衣机&#x27;</span>, <span class="string">&#x27;海尔智能门锁&#x27;</span>, <span class="string">&#x27;海尔智能音箱&#x27;</span>, <span class="string">&#x27;海尔净水器&#x27;</span>, <span class="string">&#x27;格力空气循环扇&#x27;</span>, <span class="string">&#x27;碧然德滤水壶&#x27;</span>, <span class="string">&#x27;苏泊尔电压力锅&#x27;</span>, <span class="string">&#x27;苏泊尔电饭煲&#x27;</span>&#125;,</span><br><span class="line"><span class="string">&#x27;获得的需求值&#x27;</span>: <span class="number">58</span>&#125;</span><br></pre></td></tr></table></figure><p>如果现在我又有几件商品想要购买，那么可以继续加入表格中，从新添加的行开始，照上面的方法继续计算，最终仍然可以得到最优解，而且即使打乱各行的顺序，即可选商品加入商品清单的顺序变化，虽然重新计算过程当中每次计算的结果和之前不同，但是当完成整个计算后，最终结果仍然和之前的计算结果一致。</p><p>动态规划并不是万能的，在这里我没有考虑各个商品之间的相互影响，比如说，如果我选择购买净水器，那么滤水壶的需求值就要降低，因为这两样商品的功能重复了。<strong>仅当每个子问题都是离散的，即不依赖于其他子问题时，动态规划才管用。</strong>并且使用动态规划解决类似购物问题时，要么买要么不买，假如你要买的是大米、小麦等等可以拆开只买一部分的东西，动态规划也没法解决。</p><p>总结起来，动态规划具有的特点是：</p><ol><li>需要在给定约束条件下优化某种指标时</li><li>问题可以分解为离散的子问题时</li><li>每种动态规划解决方案都涉及网格</li><li>单元格中的值通常就是需要优化的值</li><li>每个单元格都是一个子问题，因此需要考虑如何将问题分解为一个个子问题</li><li>没有普遍使用的计算动态规划解决方案的公式</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近刚装修完新房，新家种种东西方方面面都亟待采买，正好还有两个多月便是 618，打算趁着促销活动，购置一批生活家电。&lt;/p&gt;
&lt;p&gt;看着各大网购平台上琳琅满目的商品，各种宣传导购、直播带货，感觉每样东西不是吊炸天便是亮瞎眼，恨不能来者不拒，买买买…，奈何囊中羞涩，为了不至于“一波买买买，稀饭喝到腿发软”，只好提前计划，理性消费。&lt;/p&gt;
&lt;p&gt;首先规划好消费总额，暂定为只使用信用卡额度的 70%。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;TOTAL_ASSETS = &lt;span class=&quot;number&quot;&gt;10000&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 总资产&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RATE = &lt;span class=&quot;number&quot;&gt;0.7&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 使用百分比&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;AVAILABLE_ASSET = TOTAL_ASSETS*RATE  &lt;span class=&quot;comment&quot;&gt;# 可用资产&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;接下来需要列出商品清单，这里要考虑好以什么作为参考来判断最后得出的购买清单是最优的，以及以什么作为参数来选择商品。&lt;/p&gt;
&lt;p&gt;首先，既然限制了消费总额，因此使用商品的价格来作为选择与否的依据；其次，对于每样商品，会有一个需求度的期望值，也就是说该商品是否急需使用。举个例子，如果将需求度按 1 到 10 来划分，冰箱的需求度是 10(家里面总是会有东西需要冷藏)，而破壁机的需求度是 5(平时做点鲜果汁、热豆浆什么的，营养又健康，但是没有的话对生活影响不会太大)。那么，最后的购买清单便以尽可能达到最大需求度为最优解。&lt;/p&gt;
&lt;p&gt;下面列出商品清单：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;COMMODITY_LIST = &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;苏泊尔电饭煲&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;319&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;海尔净水器&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;1188&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;海尔智能门锁&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;1399&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;九阳破壁机&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;449&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;海尔智能音箱&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;199&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;海尔冰箱&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;3199&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;海尔滚筒洗衣机&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;2699&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;格力空气循环扇&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;399&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;苏泊尔电压力锅&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;259&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;),&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;碧然德滤水壶&amp;#x27;&lt;/span&gt;: (&lt;span class=&quot;number&quot;&gt;219&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;动态规划的原理是从小问题着手，然后逐步解决大问题。在这里，可以对总金额进行划分，比如先假设只能使用 100 块，然后找出当前的最佳购买方式，接着再假设只能使用 200 块，此时已经知道了使用 100 块时的最佳购买方式，只需要在此基础上看看使用 200 块能多买到什么东西，然后和上一次的计算结果进行比较，就可以轻松得出可以使用 200 块时的最佳购买方式，以此类推。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://christopher-teng.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用Scrapy时提取一个块级标签下的所有文本的正确方式</title>
    <link href="https://christopher-teng.github.io/2021/04/05/%E4%BD%BF%E7%94%A8Scrapy%E6%97%B6%E6%8F%90%E5%8F%96%E4%B8%80%E4%B8%AA%E5%9D%97%E7%BA%A7%E6%A0%87%E7%AD%BE%E4%B8%8B%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E6%9C%AC%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/"/>
    <id>https://christopher-teng.github.io/2021/04/05/%E4%BD%BF%E7%94%A8Scrapy%E6%97%B6%E6%8F%90%E5%8F%96%E4%B8%80%E4%B8%AA%E5%9D%97%E7%BA%A7%E6%A0%87%E7%AD%BE%E4%B8%8B%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E6%9C%AC%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/</id>
    <published>2021-04-05T05:33:46.000Z</published>
    <updated>2021-05-20T13:21:39.795Z</updated>
    
    <content type="html"><![CDATA[<p>使用 Scrapy 爬取一篇博客的内容(<a href="https://www.cnblogs.com/flashsun/p/14266148.html">原文链接</a>)，这里忽略正文中的图片，只针对文本内容。</p><p>博客正文内容位于标签：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;cnblogs_post_body&quot;</span> <span class="attr">class</span>=<span class="string">&quot;blogpost-body blogpost-body-html&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">strong</span>&gt;</span>你是<span class="tag">&lt;/<span class="name">strong</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">strong</span>&gt;</span>一台电脑，你的名字叫 A<span class="tag">&lt;/<span class="name">strong</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span>&gt;</span></span><br><span class="line">      如果你纠结，要么去研究一下操作系统是如何处理网络 IO</span><br><span class="line">      的，要么去研究一下包是如何被网卡转换成电信号发送出去的，要么就仅仅把它当做电脑里有个小人在</span><br><span class="line">      <span class="tag">&lt;<span class="name">strong</span>&gt;</span>开枪<span class="tag">&lt;/<span class="name">strong</span>&gt;</span></span><br><span class="line">      吧~</span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span>&gt;</span>第三层<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  ...... ......</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可见，正文内容内部的结构比较复杂，没有一致可循的规律，那么怎样提取其中所有的文本内容呢？</p><span id="more"></span><p>第一步，先选择 id=cnblogs_post_body 的 div 标签，</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;cnblogs_post_body&quot;</span> <span class="attr">class</span>=<span class="string">&quot;blogpost-body blogpost-body-html&quot;</span>&gt;</span>...<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>打开终端，使用 Scrapy Shell 测试，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell https://www.cnblogs.com/flashsun/p/14266148.html</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; post_body=response.xpath(<span class="string">&#x27;//div[@id=&quot;cnblogs_post_body&quot;]&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt; post_body</span><br><span class="line">&gt;&gt;&gt; [&lt;Selector xpath=<span class="string">&#x27;//div[@id=&quot;cnblogs_post_body&quot;]&#x27;</span> data=<span class="string">&#x27;&lt;div id=&quot;cnblogs_post_body&quot; class=&quot;bl...&#x27;</span>&gt;]</span><br></pre></td></tr></table></figure><p>然后，尝试使用 text()和 getall()获取所有文本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; post_body.xpath(<span class="string">&#x27;.//text()&#x27;</span>).getall()</span><br><span class="line">&gt;&gt;&gt; [<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;你是&#x27;</span>, <span class="string">&#x27;一台电脑，你的名字叫 A&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;很久很久之前，你不与任何其他电脑相连接，孤苦伶仃。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;直到有一天，你</span></span><br><span class="line"><span class="string">希望与另一台电脑 B 建立通信，于是你们各开了一个网口，用一根&#x27;</span>, <span class="string">&#x27;网线&#x27;</span>, <span class="string">&#x27;连接了起来。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;用一根网线连接起来怎么就能&quot;通信&quot;了呢？我可以给你讲 IO、讲中断、讲缓冲区，但这不是研究网络时该关心的问题。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;如果你纠结，要么去研究一下操作系统是如何处理网络 IO 的，要么去研究一下包是如何被网卡转换成电信号发送出去的，要么就仅仅把它当做电脑里有个小人在&#x27;</span>, <span class="string">&#x27;开枪&#x27;</span>, <span class="string">&#x27;吧~&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;反正，你们就是连起来了，并且可以通信。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;第一层&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;有一天，一个新伙伴 C 加入了，但聪明的你们很快发现，可以每个人开&#x27;</span>, <span class="string">&#x27;两个网口&#x27;</span>, <span class="string">&#x27;，用一共&#x27;</span>, <span class="string">&#x27;三根网线&#x27;</span>, <span class="string">&#x27;，彼此相连。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;随着越来越多的人加入，你发现身上开的网口实在太多了，而且网线密密麻麻，混乱不堪。（而实际上一台电脑根本开不了这么多网口，所以这种连线只在理论上可行，所以连不上的</span></span><br><span class="line"><span class="string">我就用红色虚线表示了，就是这么严谨哈哈~）&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;于是你们发明了一个中间设备，你们将网线都插到这个设备上，由这个设备做转发，就可以彼此之间通信了，本质上和原来一样，只不过网口的数量和网线的数量减少了，不再那么混乱。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;你给它取名叫&#x27;</span>, <span class="string">&#x27;集线器&#x27;</span>, <span class="string">&#x27;，它仅仅是无脑将电信号&#x27;</span>, <span class="string">&#x27;转发到所有出口（广播）&#x27;</span>, <span class="string">&#x27;，不做任何处理，你觉得它是没有智商的，因此把人家定性在了&#x27;</span>, <span class="string">&#x27;物理层&#x27;</span>, <span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;由于转发到了所有出口，那 BCDE 四台机器怎么知道数据包是不是发给自己的呢？&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;首先，你要给所有的连接到交换机的设备，都起个名字。原来你们叫 ABCD，但现在需要一个更专业的，&#x27;</span>, <span class="string">&#x27;全局唯一&#x27;</span>, <span class="string">&#x27;的名字作为标识，你把这个更高端的名字称为\xa0&#x27;</span>, <span class="string">&#x27;MAC 地址&#x27;</span>, <span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;你的 MAC 地址是 aa-aa-aa-aa-aa-aa，你的伙伴 b 的 MAC 地址是 bb-bb-bb-bb-bb-bb，以此类推，不重复就好。&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;这样，A 在发送数据包给 B 时，只要在头部拼接</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">......]</span></span><br></pre></td></tr></table></figure><p>得到结果是一个所有文本的列表，或者说是当前 div 元素下所有文本节点的集合，那么怎样才能将结果拼接成一篇完整的博客内容呢？</p><p>最直接暴力的办法当然是通过循环迭代将列表中所有元素拼接起来，这里不在赘述。其实在 XPath 中原生提供了 string()函数来转换字符串，下面尝试使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; post_body.xpath(<span class="string">&#x27;string(.//text())&#x27;</span>).getall()</span><br><span class="line">&gt;&gt;&gt; [<span class="string">&#x27;\n&#x27;</span>]</span><br></pre></td></tr></table></figure><p>什么鬼？！好好的一大篇文章就剩下一个换行……</p><p>一开始我也只好倒回去老老实实写 for…in…，直到后来翻看官方文档才找到问题所在：</p><blockquote><p>When you need to use the text content as argument to an XPath string function, avoid using .//text() and use just . instead.</p></blockquote><p>当我们使用.//text()时，所得到的是一个 node-set，把一个 node-set 传递给 string()进行字符串转换的时候，只会作用于 node-set 中的第一个元素，类似的情况还有使用 contains()和 starts-width()，而这里 post_body 中的第一个元素正好就是”\n”！</p><p>而如果将一个 node 进行字符串转换，就会将该节点以及其子孙节点中的文本进行拼接，因此在这个例子中，我们因该直接对 post_body 使用 string()：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; post_body.xpath(<span class="string">&#x27;string(.)&#x27;</span>).getall()</span><br><span class="line">&gt;&gt;&gt; [<span class="string">&#x27;\n\xa0\n你是一台电脑，你的名字叫 A\n\xa0\n很久很久之前，你不与任何其他电脑相连接，孤苦伶仃。\n\n直到有一天，你希望与另一台电脑 B 建立通信，于是你们各开了一个网口，用一根网线连接了起来。\n\n\xa0\n用一根网线连接起来怎么就能&quot;通信&quot;了呢？我可以给你讲 IO、讲中断、讲缓冲区，但这不是研究网络时该关心的问题。\n如果你纠结，要么去研究一下操作系统是如何处理网络 IO 的，要么去研究一下包是如何被网卡转换成电信号发送出去的，要么就仅仅把它当做电脑里有个小人在开枪吧~\n\n反正，你们就是连起来了，并且可以通信。\n\xa0\n第一层\n\xa0\n有一天，一个新伙伴 C 加入了，但聪明的你们很快发现，可以每个人开两个网口，用一共三根网线，彼此相连。\n\n随着越来越多的人加入，你发现身上开的网口实在太多了，而且网线密密麻麻，混乱不堪。（而实际上一台电脑根本开不了这么多网口，所以这种连线只在理论上可行，所以连不上的我就</span></span><br><span class="line"><span class="string">用红色虚线表示了，就是这么严谨哈哈~）\n\n于是你们发明了一个中间设备，你们将网线都插到这个设备上，由这个设备做转发，就可以彼此之间通信了，本质上和原来一样，只不过网口的数量和网线的数量减少了，不再那么混乱。\n\n你给它取名叫集线器，它仅仅是无脑将电信号转发到所有出口（广播），不做任何处理，你觉得它是没有智商的，因此</span></span><br><span class="line"><span class="string">把人家定性在了物理层。\n\xa0\n由于转发到了所有出口，那 BCDE 四台机器怎么知道数据包是不是发给自己的呢？\n首先，你要给所有的连接到交换机的设备，都起个名字。原</span></span><br><span class="line"><span class="string">来你们叫 ABCD，但现在需要一</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">，于是收下了这个包\n\xa0\n更详细且精准的过程：\n读到这相信大家已经很累了，理解上述过程基本上网络层以下的部分主流程就基本疏通了，如果你想要本过程更为专业的过程描述，可以在公众号 低并发编程 后台回复 网络，获得我模拟这个过程的 Cisco Packet Tracer 源文件。\n\n每一步包的传输都 会有各层的原始数据，以及专业的过程描述\n\n\xa0\n\xa0\n同时在此基础之上你也可以设计自己的网络拓扑结构，进行各种实验，来加深网络传输过程的理解。\n\xa0\n后记\n\xa0\n至此，经过物理层、数据链路层、网络层这前三层的协议，以及根据这些协议设计的各种网络设备（网线、集线器、交换机、路由器），理论上只要拥有对方的 IP 地址，就已经将地球上任意位置的两个节点连通了。\n\n本文经过了很多次的修改，删减了不少影响主流程的内容，就是为了让读者能抓住网络传输前三层的真正核心思想。同时网络相关 的知识也是多且杂，我也还有很多搞不清楚的地方，非常欢迎大家与我交流，共同进步。\n&#x27;</span>]</span><br></pre></td></tr></table></figure><p>哈哈，这次终于正确返回了一篇完整的博客文本内容！当然，在这里我们可以直接使用 get()而不是 getall()，因为 string(.)会自动将当前节点和其子孙节点的文本进行拼接，这样就可以获得一个字符串方便直接使用。</p><p>在官方文档中还列举了一个使用 contains()对节点文本内容进行匹配的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from scrapy import Selector</span><br><span class="line">&gt;&gt;&gt; sel=Selector(text=<span class="string">&#x27;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt; sel.xpath(<span class="string">&#x27;//a[contains(.//text(),&#x27;</span>Next Page<span class="string">&#x27;)]&#x27;</span>).getall()</span><br><span class="line">&gt;&gt;&gt; []</span><br><span class="line">&gt;&gt;&gt; sel.xpath(<span class="string">&#x27;//a[contains(.,&#x27;</span>Next Page<span class="string">&#x27;)]&#x27;</span>).getall()</span><br><span class="line">&gt;&gt;&gt; [<span class="string">&#x27;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure><p>使用.//text()时，得到的结果其实只有 a 中的第一个文本节点，即：”Click here to go to the “，因此无法匹配文本”Next Page”，而使用节点本身，得到的是当前节点和其子孙节点的所有文本内容的拼接，即：”Click here to go to the Next Page”，因此成功匹配。</p><p>最后，放上 Scrapy 官方文档中，本文相关内容的<a href="https://docs.scrapy.org/en/latest/topics/selectors.html#using-text-nodes-in-a-condition">链接</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用 Scrapy 爬取一篇博客的内容(&lt;a href=&quot;https://www.cnblogs.com/flashsun/p/14266148.html&quot;&gt;原文链接&lt;/a&gt;)，这里忽略正文中的图片，只针对文本内容。&lt;/p&gt;
&lt;p&gt;博客正文内容位于标签：&lt;/p&gt;
&lt;figure class=&quot;highlight html&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;id&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;cnblogs_post_body&amp;quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&amp;quot;blogpost-body blogpost-body-html&amp;quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;span&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;你是&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;一台电脑，你的名字叫 A&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;span&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;span&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      如果你纠结，要么去研究一下操作系统是如何处理网络 IO&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      的，要么去研究一下包是如何被网卡转换成电信号发送出去的，要么就仅仅把它当做电脑里有个小人在&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;开枪&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;strong&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      吧~&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;span&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;h1&lt;/span&gt;&amp;gt;&lt;/span&gt;第三层&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;h1&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;...&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;p&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ...... ......&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;可见，正文内容内部的结构比较复杂，没有一致可循的规律，那么怎样提取其中所有的文本内容呢？&lt;/p&gt;</summary>
    
    
    
    <category term="网络爬虫" scheme="https://christopher-teng.github.io/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
    <category term="Scrapy" scheme="https://christopher-teng.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>网络爬虫中常用的数据提取工具</title>
    <link href="https://christopher-teng.github.io/2021/04/05/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E5%B7%A5%E5%85%B7/"/>
    <id>https://christopher-teng.github.io/2021/04/05/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E5%B7%A5%E5%85%B7/</id>
    <published>2021-04-04T18:08:23.000Z</published>
    <updated>2021-05-20T13:21:39.815Z</updated>
    
    <content type="html"><![CDATA[<p>在网络爬虫开发中，需要对返回的响应内容进行分析提取，常见的工具主要有 Beautiful Soup、PyQuery 以及框架 Scrapy 自带的 Selector，他们之间有何区别以及新上手开发爬虫该如何选择呢？</p><h3 id="Beautiful-Soup-和-pyQuery"><a href="#Beautiful-Soup-和-pyQuery" class="headerlink" title="Beautiful Soup 和 pyQuery"></a>Beautiful Soup 和 pyQuery</h3><p>Beautiful Soup 目前最新版本为 bs4，和 PyQuery 对比，在使用中二者语法大同小异，如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Beautiful Soup4</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> BlogsItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">    soup=BeautifulSoup(response.body,<span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    item=BlogsItem()</span><br><span class="line">    item[<span class="string">&#x27;title&#x27;</span>]=soup.select(<span class="string">&#x27;#mainContent .postTitle2.vertical-middle&#x27;</span>).text()</span><br><span class="line">    item[<span class="string">&#x27;description&#x27;</span>]=soup.select(<span class="string">&#x27;#mainContent .c_b_p_desc).text()</span></span><br><span class="line"><span class="string">    item[&#x27;</span>pub_date<span class="string">&#x27;]=soup.select(&#x27;</span><span class="comment">#mainContent .dayTitle&gt;span).text()</span></span><br></pre></td></tr></table></figure><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyQuery</span></span><br><span class="line"><span class="keyword">from</span> pyQuery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> BlogsItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self,response</span>):</span></span><br><span class="line">    doc=pq(response.body)</span><br><span class="line">    item=BlogsItem()</span><br><span class="line">    item[<span class="string">&#x27;title&#x27;</span>]=doc(<span class="string">&#x27;#mainContent .postTitle2.vertical-middle&#x27;</span>).text()</span><br><span class="line">    item[<span class="string">&#x27;description&#x27;</span>]=doc(<span class="string">&#x27;#mainContent .c_b_p_desc&#x27;</span>).text()</span><br><span class="line">    item[<span class="string">&#x27;pub_date&#x27;</span>]=doc(<span class="string">&#x27;#mainContent .dayTitle&gt;span&#x27;</span>).text()</span><br></pre></td></tr></table></figure><p>相对于 PyQuery 而言，Beautiful Soup 年代更久远，成熟度更高，文档完善，但是其性能较差，尤其在爬取任务量巨大(爬取量在万级以上)时，会有比较明显的速度滞后感，而且对内存的消耗也比较大。PyQuery 比 bs4 性能要更好一些，而且其提供类似 jQuery 的语法，对于熟悉前端技术的用户来说非常容易上手。</p><h3 id="Scrapy-Selector"><a href="#Scrapy-Selector" class="headerlink" title="Scrapy Selector"></a>Scrapy Selector</h3><p>除了上述两种流行的第三方库，在爬虫框架 Scrapy 中还原生提供了两种选择器：XPath 和 CSS。</p><ol><li><p>XPath</p><p>XPath，全称 XML Path Language，即 XML 路径语言。XPath 用于在 XML 文档中通过元素和属性进行导航，同样适用于 HTML。</p><p>XPath 使用路径表达式在文档中进行导航，语法简介明了，并且包含有超过 100 个内建函数用于字符串、数值、日期和时间比较，以及节点处理、序列处理、逻辑值判断等，XPath 也是一个 W3C 标准。</p><p>使用 XPath 几乎可以准确定位所有节点。</p><p>XPath 的常用规则主要是：</p><ul><li>nodename：选取此节点的所有子节点</li><li>/：从当前节点选取直接子节点</li><li>//：从当前节点选取子孙节点</li><li>.：选取当前节点</li><li>..：选取当前节点的父节点</li><li>@：选取属性，也可以进行属性匹配，如 div[@id=”post_title”]</li><li>text()：获取文本</li><li>contains()：属性多值匹配，如 div[contains(@id,”post_title”)]</li><li>[and]：多属性匹配，如 div[contains(@id,”post_title”) and @class=”post-item”]</li></ul><p>XPath 的常用运算符：</p><ul><li>or：逻辑或</li><li>and：逻辑与</li><li>mod：求余数</li><li>|：求节点集</li><li>+/-/*/div：加减乘除</li><li>=/!=：等于、不等于</li><li>&lt; /&lt;= / &gt; / &gt;=：小于、小于等于，大于，大于等于</li></ul><p>XPath 还支持按序选择：</p><ul><li>‘//li[1]/text()’：获取第一个 li</li><li>‘//li[last()]/text()’：获取最后一个 li</li><li>‘//li[position()&lt;3]/text()’：获取前 2 个 li</li><li>‘//li[last()-2]/text()’：获取倒数第二个 li</li></ul><p>更多的使用方法可以查阅 XPath 相关文档，下面是一个使用示例，从<a href="https://www.cnblogs.com/">博客园</a>的<a href="https://www.cnblog.com/pick/">精华</a>频道爬取文章标题、内容摘要、作者和发布时间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogsSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;blogs&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;zzk.cnblogs.com/&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.cnblogs.com/pick/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># use xpath selector</span></span><br><span class="line">        <span class="keyword">for</span> post <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//div[@id=&quot;post_list&quot;]//article[has-class(&quot;post-item&quot;)]&#x27;</span>):</span><br><span class="line">            title = post.xpath(<span class="string">&#x27;.//a[has-class(&quot;post-item-title&quot;)]/text()&#x27;</span>).get()</span><br><span class="line">            link = post.xpath(<span class="string">&#x27;.//a[has-class(&quot;post-item-title&quot;)]/@href&#x27;</span>).get()</span><br><span class="line">            summary = post.xpath(<span class="string">&#x27;.//p[has-class(&quot;post-item-summary&quot;)]/text()&#x27;</span>).getall()[-<span class="number">1</span>].strip()</span><br><span class="line">            author = post.xpath(<span class="string">&#x27;.//a[has-class(&quot;post-item-author&quot;)]/span/text()&#x27;</span>).get()</span><br><span class="line">            publish = post.xpath(<span class="string">&#x27;.//span[has-class(&quot;post-meta-item&quot;)]/span/text()&#x27;</span>).get()</span><br><span class="line">            next_page_url = response.xpath(<span class="string">&#x27;//div[@id=&quot;pager_bottom&quot;]//a/@href&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">&#x27;文章标题&#x27;</span>: title, <span class="string">&#x27;原文链接&#x27;</span>: link, <span class="string">&#x27;内容摘要&#x27;</span>: summary, <span class="string">&#x27;作者&#x27;</span>: author, <span class="string">&#x27;发布时间&#x27;</span>: publish&#125;</span><br><span class="line">            <span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> response.follow(next_page_url, callback=self.parse)</span><br></pre></td></tr></table></figure><p>部分爬取结果如图：</p><img src="/2021/04/05/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E5%B7%A5%E5%85%B7/result.png" class=""></li><li><p>CSS</p><p>如果你不懂 XPath 语法，还可以使用 CSS 选择器，CSS 选择器拥有接近于 XPath 选择器的速度，因为 Scrapy 会将 CSS 选择器转换为 XPath。在 scrapy.Selector.css 方法中会调用 Selector 的内部方法_css2xpath，这是一个自动将 css 选择器语法转换成 xpath 语法的私有方法。</p><p>下面是使用 css 选择器语法改写上面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogsSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;blogs&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;zzk.cnblogs.com/&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.cnblogs.com/pick/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># use css selector</span></span><br><span class="line">        <span class="keyword">for</span> post <span class="keyword">in</span> response.css(<span class="string">&#x27;div#post_list article.post-item&#x27;</span>):</span><br><span class="line">            title = post.css(<span class="string">&#x27;a.post-item-title::text&#x27;</span>).get()</span><br><span class="line">            link = post.css(<span class="string">&#x27;a.post-item-title::attr(href)&#x27;</span>).get()</span><br><span class="line">            summary = post.css(<span class="string">&#x27;p.post-item-summary::text&#x27;</span>).getall()[-<span class="number">1</span>].strip()</span><br><span class="line">            author = post.css(<span class="string">&#x27;a.post-item-author&gt;span::text&#x27;</span>).get()</span><br><span class="line">            publish = post.css(<span class="string">&#x27;span.post-meta-item&gt;span::text&#x27;</span>).get()</span><br><span class="line">            next_page_url = response.css(<span class="string">&#x27;div#pager_bottom a::attr(href)&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">&#x27;文章标题&#x27;</span>: title, <span class="string">&#x27;原文链接&#x27;</span>: link, <span class="string">&#x27;内容摘要&#x27;</span>: summary, <span class="string">&#x27;作者&#x27;</span>: author, <span class="string">&#x27;发布时间&#x27;</span>: publish&#125;</span><br><span class="line">            <span class="keyword">if</span> next_page_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> response.follow(next_page_url, callback=self.parse)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>如果使用 Scrapy 进行爬虫开发，推荐使用 Scrapy 提供的 XPath 选择器或者 CSS 选择器，无需额外引入第三方库，并且速度很快。而在 Beautiful Soup 和 PyQuery 之间，可以选择性能更好的 PyQuery，对于有 jQuery 使用经验的用户来说尤其方便快速上手。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在网络爬虫开发中，需要对返回的响应内容进行分析提取，常见的工具主要有 Beautiful Soup、PyQuery 以及框架 Scrapy 自带的 Selector，他们之间有何区别以及新上手开发爬虫该如何选择呢？&lt;/p&gt;
&lt;h3 id=&quot;Beautiful-Soup-和-pyQuery&quot;&gt;&lt;a href=&quot;#Beautiful-Soup-和-pyQuery&quot; class=&quot;headerlink&quot; title=&quot;Beautiful Soup 和 pyQuery&quot;&gt;&lt;/a&gt;Beautiful Soup 和 pyQuery&lt;/h3&gt;&lt;p&gt;Beautiful Soup 目前最新版本为 bs4，和 PyQuery 对比，在使用中二者语法大同小异，如下代码所示：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Beautiful Soup4&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; bs4 &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; BeautifulSoup&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; ..items &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; BlogsItem&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;parse_item&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self,response&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    soup=BeautifulSoup(response.body,&lt;span class=&quot;string&quot;&gt;&amp;#x27;html.parser&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    item=BlogsItem()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    item[&lt;span class=&quot;string&quot;&gt;&amp;#x27;title&amp;#x27;&lt;/span&gt;]=soup.select(&lt;span class=&quot;string&quot;&gt;&amp;#x27;#mainContent .postTitle2.vertical-middle&amp;#x27;&lt;/span&gt;).text()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    item[&lt;span class=&quot;string&quot;&gt;&amp;#x27;description&amp;#x27;&lt;/span&gt;]=soup.select(&lt;span class=&quot;string&quot;&gt;&amp;#x27;#mainContent .c_b_p_desc).text()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;    item[&amp;#x27;&lt;/span&gt;pub_date&lt;span class=&quot;string&quot;&gt;&amp;#x27;]=soup.select(&amp;#x27;&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;#mainContent .dayTitle&amp;gt;span).text()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="网络爬虫" scheme="https://christopher-teng.github.io/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://christopher-teng.github.io/tags/Python/"/>
    
    <category term="Scrapy" scheme="https://christopher-teng.github.io/tags/Scrapy/"/>
    
    <category term="Beautiful Soup" scheme="https://christopher-teng.github.io/tags/Beautiful-Soup/"/>
    
    <category term="PyQuery" scheme="https://christopher-teng.github.io/tags/PyQuery/"/>
    
  </entry>
  
</feed>
